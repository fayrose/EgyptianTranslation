{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EgyptianTranslation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtE9rzCj0GNPwjXduMIcBy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ShPLbx1GiY",
        "outputId": "6613ce3f-b39a-4481-f58b-e5c855cc52cf"
      },
      "source": [
        "!git clone https://github.com/fayrose/EgyptianTranslation.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EgyptianTranslation'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 143 (delta 34), reused 143 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (143/143), 5.88 MiB | 18.88 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrdeqEco2G03",
        "outputId": "fbc49b69-bfff-447e-ba64-e676d5c2d004"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/23/c565e03ddffb57db1b79bd9a97c8f56895eea094d9314ba5b12ce1282593/OpenNMT_py-2.1.2-py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 7.5MB/s \n",
            "\u001b[?25hCollecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 24kB/s \n",
            "\u001b[?25hCollecting configargparse<2,>=1.2.3\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/47/9afae827a2159bb676fdc1d65c36539968c52031584f443f1938b939f23b/ConfigArgParse-1.5-py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.5.0)\n",
            "Collecting flask==1.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23; platform_system == \"Linux\" or platform_system == \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/0c/bad1d4c1c42885f89ef7e9697534af17906e59fc6703872b2a3a52d7c06f/pyonmttok-1.26.2-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n",
            "\u001b[K     |████████████████████████████████| 14.3MB 258kB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.9MB/s \n",
            "\u001b[?25hCollecting waitress==1.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hCollecting tqdm<5,>=4.51\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/20/9f1e974bb4761128fc0d0a32813eaa92827309b1756c4b892d28adfb4415/tqdm-4.61.1-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
            "\u001b[?25hCollecting pyyaml==5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/86/83ae5504903b4eca5ae1fd3c53b87b640f9c302df2c97fc08331ec7f3c8a/PyYAML-5.4-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py) (1.19.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (57.0.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (7.1.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2021.5.30)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py) (2.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (3.1.1)\n",
            "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, configargparse, flask, pyonmttok, sentencepiece, tqdm, torchtext, waitress, pyyaml, OpenNMT-py\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed OpenNMT-py-2.1.2 configargparse-1.5 flask-1.1.2 pyonmttok-1.26.2 pyyaml-5.4 sentencepiece-0.1.96 torch-1.6.0 torchtext-0.5.0 tqdm-4.61.1 waitress-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofRY0VT2V37"
      },
      "source": [
        "string = \"\"\"\n",
        "# egy_eng.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: EgyptianTranslation/onmt-files/run/example\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: EgyptianTranslation/onmt-files/run/example.vocab.egy\n",
        "tgt_vocab: EgyptianTranslation/onmt-files/run/example.vocab.eng\n",
        "# Prevent overwriting existing files in the folder\n",
        "overwrite: True\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: EgyptianTranslation/compiled_corpora/aligned_train.egy.csv\n",
        "        path_tgt: EgyptianTranslation/compiled_corpora/aligned_train.eng.csv\n",
        "    valid:\n",
        "        path_src: EgyptianTranslation/compiled_corpora/aligned_val.egy.csv\n",
        "        path_tgt: EgyptianTranslation/compiled_corpora/aligned_val.eng.csv\n",
        "\"\"\"\n",
        "with open(\"egy_eng.yaml\", 'w') as yaml_file:\n",
        "  yaml_file.write(string)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCW19tnM3G31",
        "outputId": "03336cd2-a1d2-432c-b28f-1aaf12901c29"
      },
      "source": [
        "!onmt_build_vocab -config egy_eng.yaml -n_sample 10000"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-06-24 06:35:12,268 INFO] Counter vocab from 10000 samples.\n",
            "[2021-06-24 06:35:12,268 INFO] Build vocab on 10000 transformed examples/corpus.\n",
            "[2021-06-24 06:35:12,276 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-06-24 06:35:12,276 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:12,377 INFO] Counters src:2579\n",
            "[2021-06-24 06:35:12,377 INFO] Counters tgt:4201\n",
            "[2021-06-24 06:35:12,377 WARNING] path EgyptianTranslation/onmt-files/run/example.vocab.egy exists, may overwrite...\n",
            "[2021-06-24 06:35:12,380 WARNING] path EgyptianTranslation/onmt-files/run/example.vocab.eng exists, may overwrite...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFzWt6Nt3Vyy"
      },
      "source": [
        "string = \"\"\"\n",
        "# Vocabulary files that were just created\n",
        "src_vocab: EgyptianTranslation/onmt-files/run/example.vocab.egy\n",
        "tgt_vocab: EgyptianTranslation/onmt-files/run/example.vocab.eng\n",
        "\n",
        "# Train on a single GPU\n",
        "world_size: 1\n",
        "\n",
        "# Where to save the checkpoints\n",
        "save_model: EgyptianTranslation/onmt-files/run/model\n",
        "save_checkpoint_steps: 1000\n",
        "train_steps: 3000\n",
        "valid_steps: 1500\n",
        "\n",
        "# Optimization\n",
        "# model_dtype: \"fp32\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "label_smoothing: 0.1\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "# enc_layers: 6\n",
        "# dec_layers: 6\n",
        "position_encoding: true\n",
        "heads: 8\n",
        "rnn_size: 512\n",
        "word_vec_size: 512\n",
        "max_generator_batches: 2\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "accum_count: 3\n",
        "attention_dropout: [0.1]\n",
        "# share_decoder_embeddings: true\n",
        "# share_embeddings: true\n",
        "\"\"\"\n",
        "\n",
        "with open(\"egy_eng.yaml\", 'a') as yaml_file:\n",
        "  yaml_file.write(string)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1l3ulnX_uTw",
        "outputId": "e74933de-de7b-4e52-9b15-f18f130f2fd8"
      },
      "source": [
        "!onmt_train --config egy_eng.yaml -share_vocab -gpu_ranks 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-24 06:35:20,719 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-06-24 06:35:20,719 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-06-24 06:35:20,719 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-06-24 06:35:20,719 INFO] Parsed 2 corpora from -data.\n",
            "[2021-06-24 06:35:20,719 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-06-24 06:35:20,719 INFO] Loading vocab from text file...\n",
            "[2021-06-24 06:35:20,719 INFO] Loading src vocabulary from EgyptianTranslation/onmt-files/run/example.vocab.egy\n",
            "[2021-06-24 06:35:20,727 INFO] Loaded src vocab has 2579 tokens.\n",
            "[2021-06-24 06:35:20,728 INFO] Loading tgt vocabulary from EgyptianTranslation/onmt-files/run/example.vocab.eng\n",
            "[2021-06-24 06:35:20,739 INFO] Loaded tgt vocab has 4201 tokens.\n",
            "[2021-06-24 06:35:20,741 INFO] Building fields with vocab in counters...\n",
            "[2021-06-24 06:35:20,746 INFO]  * tgt vocab size: 4205.\n",
            "[2021-06-24 06:35:20,748 INFO]  * src vocab size: 2581.\n",
            "[2021-06-24 06:35:20,748 INFO]  * merging src and tgt vocab...\n",
            "[2021-06-24 06:35:20,758 INFO]  * merged vocab size: 6684.\n",
            "[2021-06-24 06:35:20,758 INFO]  * src vocab size = 6684\n",
            "[2021-06-24 06:35:20,759 INFO]  * tgt vocab size = 6684\n",
            "[2021-06-24 06:35:20,760 INFO] Building model...\n",
            "[2021-06-24 06:35:24,172 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6684, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6684, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=6684, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-06-24 06:35:24,173 INFO] encoder: 9728000\n",
            "[2021-06-24 06:35:24,173 INFO] decoder: 15260188\n",
            "[2021-06-24 06:35:24,173 INFO] * number of parameters: 24988188\n",
            "[2021-06-24 06:35:24,174 INFO] Starting training on GPU: [0]\n",
            "[2021-06-24 06:35:24,174 INFO] Start training loop and validate every 5000 steps...\n",
            "[2021-06-24 06:35:24,175 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-06-24 06:35:24,175 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:27,404 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:29,253 INFO] Step 50/10000; acc:   5.22; ppl: 1749.99; xent: 7.47; lr: 0.00001; 10735/14501 tok/s;      5 sec\n",
            "[2021-06-24 06:35:30,620 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:33,779 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:34,189 INFO] Step 100/10000; acc:  13.18; ppl: 868.86; xent: 6.77; lr: 0.00001; 10933/14760 tok/s;     10 sec\n",
            "[2021-06-24 06:35:36,964 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:39,177 INFO] Step 150/10000; acc:  13.13; ppl: 520.96; xent: 6.26; lr: 0.00002; 10885/14658 tok/s;     15 sec\n",
            "[2021-06-24 06:35:40,130 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:44,177 INFO] Step 200/10000; acc:  12.97; ppl: 336.76; xent: 5.82; lr: 0.00002; 11009/14826 tok/s;     20 sec\n",
            "[2021-06-24 06:35:44,356 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:47,598 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:49,184 INFO] Step 250/10000; acc:  14.51; ppl: 218.85; xent: 5.39; lr: 0.00003; 10767/14530 tok/s;     25 sec\n",
            "[2021-06-24 06:35:50,830 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:54,043 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:54,236 INFO] Step 300/10000; acc:  16.16; ppl: 157.01; xent: 5.06; lr: 0.00004; 10786/14507 tok/s;     30 sec\n",
            "[2021-06-24 06:35:57,276 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:35:59,316 INFO] Step 350/10000; acc:  17.99; ppl: 126.72; xent: 4.84; lr: 0.00004; 10755/14548 tok/s;     35 sec\n",
            "[2021-06-24 06:36:01,647 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:04,422 INFO] Step 400/10000; acc:  20.77; ppl: 103.51; xent: 4.64; lr: 0.00005; 10579/14374 tok/s;     40 sec\n",
            "[2021-06-24 06:36:04,950 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:08,242 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:09,616 INFO] Step 450/10000; acc:  23.69; ppl: 84.92; xent: 4.44; lr: 0.00006; 10473/13981 tok/s;     45 sec\n",
            "[2021-06-24 06:36:11,532 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:14,722 INFO] Step 500/10000; acc:  27.06; ppl: 66.82; xent: 4.20; lr: 0.00006; 10709/14476 tok/s;     51 sec\n",
            "[2021-06-24 06:36:14,818 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:19,292 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:19,930 INFO] Step 550/10000; acc:  30.04; ppl: 53.13; xent: 3.97; lr: 0.00007; 10448/14128 tok/s;     56 sec\n",
            "[2021-06-24 06:36:22,643 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:25,216 INFO] Step 600/10000; acc:  33.30; ppl: 42.12; xent: 3.74; lr: 0.00007; 10260/13789 tok/s;     61 sec\n",
            "[2021-06-24 06:36:26,044 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:29,358 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:30,415 INFO] Step 650/10000; acc:  35.92; ppl: 34.31; xent: 3.54; lr: 0.00008; 10521/14173 tok/s;     66 sec\n",
            "[2021-06-24 06:36:32,701 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:35,661 INFO] Step 700/10000; acc:  39.16; ppl: 27.25; xent: 3.31; lr: 0.00009; 10302/13977 tok/s;     71 sec\n",
            "[2021-06-24 06:36:37,142 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:40,540 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:40,962 INFO] Step 750/10000; acc:  41.63; ppl: 22.57; xent: 3.12; lr: 0.00009; 10250/13768 tok/s;     77 sec\n",
            "[2021-06-24 06:36:43,906 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:46,206 INFO] Step 800/10000; acc:  44.39; ppl: 18.75; xent: 2.93; lr: 0.00010; 10372/13973 tok/s;     82 sec\n",
            "[2021-06-24 06:36:47,288 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:50,607 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:51,482 INFO] Step 850/10000; acc:  47.04; ppl: 15.38; xent: 2.73; lr: 0.00011; 10387/13992 tok/s;     87 sec\n",
            "[2021-06-24 06:36:55,118 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:36:56,665 INFO] Step 900/10000; acc:  50.10; ppl: 12.70; xent: 2.54; lr: 0.00011; 10369/14026 tok/s;     92 sec\n",
            "[2021-06-24 06:36:58,436 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:01,808 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:01,906 INFO] Step 950/10000; acc:  52.45; ppl: 10.77; xent: 2.38; lr: 0.00012; 10391/14012 tok/s;     98 sec\n",
            "[2021-06-24 06:37:05,117 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:07,108 INFO] Step 1000/10000; acc:  55.00; ppl:  9.17; xent: 2.22; lr: 0.00012; 10555/14209 tok/s;    103 sec\n",
            "[2021-06-24 06:37:07,128 INFO] Saving checkpoint EgyptianTranslation/onmt-files/run/model_step_1000.pt\n",
            "[2021-06-24 06:37:09,500 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:12,773 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:13,269 INFO] Step 1050/10000; acc:  58.00; ppl:  7.63; xent: 2.03; lr: 0.00013; 8750/11875 tok/s;    109 sec\n",
            "[2021-06-24 06:37:17,217 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n",
            "[2021-06-24 06:37:18,489 INFO] Step 1100/10000; acc:  60.20; ppl:  6.68; xent: 1.90; lr: 0.00014; 10448/13979 tok/s;    114 sec\n",
            "[2021-06-24 06:37:20,544 INFO] Loading ParallelCorpus(EgyptianTranslation/compiled_corpora/aligned_train.egy.csv, EgyptianTranslation/compiled_corpora/aligned_train.eng.csv, align=None)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPZv8-MfJwUf"
      },
      "source": [
        "!onmt_translate -model EgyptianTranslation/onmt-files/run/model_step_3000.pt -src /content/EgyptianTranslation/compiled_corpora/aligned_test.egy.csv -output EgyptianTranslation/onmt-files/run/pred_1000.txt -verbose\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}