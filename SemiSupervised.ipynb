{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemiSupervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xPRcgp0Kx_aoPQPo7EP8o92ktu4Oveyi",
      "authorship_tag": "ABX9TyN9yr+J5GxbSIXBPbp/LDWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fayrose/EgyptianTranslation/blob/main/SemiSupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmjLWXZo7iWI"
      },
      "source": [
        "# Generate vocabularies to accompany embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aJckuOobq-C",
        "outputId": "89a2c3e1-d0f5-47ce-baa3-90238cc58651"
      },
      "source": [
        "%cd drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shG4N_Q_vnkb",
        "outputId": "fca94487-1d3f-469e-cca4-0ea1fa522a02"
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 2s (964 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ShPLbx1GiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d22ae91-e2e2-4e57-fd97-2bb2b39115ab"
      },
      "source": [
        "!git lfs clone https://github.com/fayrose/EgyptianTranslation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'EgyptianTranslation'...\n",
            "remote: Enumerating objects: 309, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (241/241), done.\u001b[K\n",
            "remote: Total 309 (delta 109), reused 245 (delta 65), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (309/309), 12.86 MiB | 11.00 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Git LFS: (9 of 9 files) 501.76 MB / 501.76 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrdeqEco2G03",
        "outputId": "71ef781e-6697-416b-e80b-a50b41c90a4b"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 216 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.6.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.2-py3-none-any.whl (20 kB)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.9.0+cu102)\n",
            "Collecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.28.0-cp37-cp37m-manylinux2010_x86_64.whl (15.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.8 MB 146 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.40.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py) (3.7.4.3)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.5.0)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.2 pyonmttok-1.28.0 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofRY0VT2V37"
      },
      "source": [
        "string = \"\"\"\n",
        "# egy_eng.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: /content/drive/MyDrive/EgyptianTranslation/word2vec/\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: /content/drive/MyDrive/EgyptianTranslation/word2vec/egy.vocab.txt\n",
        "share_vocab: True\n",
        "overwrite: True\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_train.egy.csv\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_val.egy.csv\n",
        "    monolingual:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/egyptian_monolingual.csv\n",
        "\"\"\"\n",
        "with open(\"EgyptianTranslation/egy_eng.yaml\", 'w') as yaml_file:\n",
        "  yaml_file.write(string)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCW19tnM3G31",
        "outputId": "e0c85f33-74f2-468c-eccf-28cbda6c14fb"
      },
      "source": [
        "!onmt_build_vocab -config EgyptianTranslation/egy_eng.yaml -min_frequency 1 -n_sample -1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path_tgt is None, it should be set unless the task is language modeling\n",
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "path_tgt is None, it should be set unless the task is language modeling\n",
            "path_tgt is None, it should be set unless the task is language modeling\n",
            "Corpus monolingual's weight should be given. We default it to 1 for you.\n",
            "[2021-09-24 17:01:09,965 INFO] Counter vocab from -1 samples.\n",
            "[2021-09-24 17:01:09,966 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2021-09-24 17:01:09,974 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-09-24 17:01:09,975 INFO] monolingual's transforms: TransformPipe()\n",
            "[2021-09-24 17:01:10,777 INFO] Counters src:8816\n",
            "[2021-09-24 17:01:10,777 INFO] Counters tgt:8816\n",
            "[2021-09-24 17:01:10,780 INFO] Counters after share:8816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "canTXjZx4a3C"
      },
      "source": [
        "string = \"\"\"\n",
        "# egy_eng.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: /content/drive/MyDrive/EgyptianTranslation/word2vec/\n",
        "## Where the vocab(s) will be written\n",
        "src_vocab: /content/drive/MyDrive/EgyptianTranslation/word2vec/eng.vocab.txt\n",
        "share_vocab: True\n",
        "overwrite: True\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_train.eng.csv\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_val.eng.csv\n",
        "\"\"\"\n",
        "with open(\"EgyptianTranslation/egy_eng.yaml\", 'w') as yaml_file:\n",
        "  yaml_file.write(string)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBHRr2-S5cvp",
        "outputId": "430ff600-1993-4545-d331-3348828a16c4"
      },
      "source": [
        "!onmt_build_vocab -config EgyptianTranslation/egy_eng.yaml -min_frequency 1 -n_sample -1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path_tgt is None, it should be set unless the task is language modeling\n",
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "path_tgt is None, it should be set unless the task is language modeling\n",
            "[2021-09-24 17:01:38,708 INFO] Counter vocab from -1 samples.\n",
            "[2021-09-24 17:01:38,708 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2021-09-24 17:01:38,719 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-09-24 17:01:38,921 INFO] Counters src:6356\n",
            "[2021-09-24 17:01:38,921 INFO] Counters tgt:6356\n",
            "[2021-09-24 17:01:38,923 INFO] Counters after share:6356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1E43cSq7nWV"
      },
      "source": [
        "#Convert embeddings to format recognized by OpenNMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFzWt6Nt3Vyy"
      },
      "source": [
        "string = \"\"\"\n",
        "# egy_eng.yaml\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: /content/drive/MyDrive/EgyptianTranslation/word2vec/out\n",
        "## Where the vocab(s) will be written\n",
        "#src_vocab: /content/drive/MyDrive/EgyptianTranslation/word2vec/eng.vocab.txt\n",
        "overwrite: True\n",
        "\n",
        "# Corpus opts:\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_train.egy.csv\n",
        "        path_tgt: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_train.eng.csv\n",
        "    valid:\n",
        "        path_src: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_val.egy.csv\n",
        "        path_tgt: /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_val.eng.csv\n",
        "\n",
        "# Vocabulary files that were just created\n",
        "src_vocab: /content/drive/MyDrive/EgyptianTranslation/word2vec/egy.vocab.txt\n",
        "tgt_vocab: /content/drive/MyDrive/EgyptianTranslation/word2vec/eng.vocab.txt\n",
        "\n",
        "# Train on a single GPU\n",
        "world_size: 1\n",
        "\n",
        "# Add embeddings\n",
        "src_embeddings: /content/drive/MyDrive/EgyptianTranslation/word2vec/txt/egy.model.txt\n",
        "tgt_embeddings: /content/drive/MyDrive/EgyptianTranslation/word2vec/txt/eng.model.txt\n",
        "embeddings_type: 'word2vec'\n",
        "\n",
        "# Where to save the checkpoints\n",
        "save_model: /content/drive/MyDrive/EgyptianTranslation/word2vec/output\n",
        "save_checkpoint_steps: 1000\n",
        "train_steps: 3000\n",
        "valid_steps: 1500\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp32\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "label_smoothing: 0.1\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "rnn_size: 512\n",
        "word_vec_size: 512\n",
        "max_generator_batches: 2\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "accum_count: 4\n",
        "attention_dropout: [0.1]\n",
        "gpu_ranks: 0\n",
        "\"\"\"\n",
        "\n",
        "with open(\"EgyptianTranslation/egy_eng.yaml\", 'w') as yaml_file:\n",
        "  yaml_file.write(string)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1l3ulnX_uTw",
        "outputId": "76e3b75f-b202-4bc5-f78b-e2c15c995a25"
      },
      "source": [
        "!onmt_train --config EgyptianTranslation/egy_eng.yaml"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-09-24 17:06:12,068 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-09-24 17:06:12,069 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-09-24 17:06:12,069 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-09-24 17:06:12,070 INFO] Parsed 2 corpora from -data.\n",
            "[2021-09-24 17:06:12,071 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-09-24 17:06:12,071 INFO] Loading vocab from text file...\n",
            "[2021-09-24 17:06:12,071 INFO] Loading src vocabulary from /content/drive/MyDrive/EgyptianTranslation/word2vec/egy.vocab.txt\n",
            "[2021-09-24 17:06:12,091 INFO] Loaded src vocab has 8816 tokens.\n",
            "[2021-09-24 17:06:12,095 INFO] Loading tgt vocabulary from /content/drive/MyDrive/EgyptianTranslation/word2vec/eng.vocab.txt\n",
            "[2021-09-24 17:06:12,110 INFO] Loaded tgt vocab has 6356 tokens.\n",
            "[2021-09-24 17:06:12,113 INFO] Building fields with vocab in counters...\n",
            "[2021-09-24 17:06:12,122 INFO]  * tgt vocab size: 6360.\n",
            "[2021-09-24 17:06:12,133 INFO]  * src vocab size: 8818.\n",
            "[2021-09-24 17:06:12,133 INFO] Reading encoder embeddings from /content/drive/MyDrive/EgyptianTranslation/word2vec/txt/egy.model.txt\n",
            "[2021-09-24 17:06:13,391 INFO] \tFound 3183 total vectors in file.\n",
            "[2021-09-24 17:06:13,391 INFO] Reading decoder embeddings from /content/drive/MyDrive/EgyptianTranslation/word2vec/txt/eng.model.txt\n",
            "[2021-09-24 17:06:15,692 INFO] \tFound 6356 total vectors in file\n",
            "[2021-09-24 17:06:15,692 INFO] After filtering to vectors in vocab:\n",
            "[2021-09-24 17:06:15,694 INFO] \t* enc: 3183 match, 5635 missing, (36.10%)\n",
            "[2021-09-24 17:06:15,696 INFO] \t* dec: 6356 match, 4 missing, (99.94%)\n",
            "[2021-09-24 17:06:15,696 INFO] \n",
            "Saving encoder embeddings as:\n",
            "\t* enc: /content/drive/MyDrive/EgyptianTranslation/word2vec/out.enc_embeddings.pt\n",
            "[2021-09-24 17:06:15,888 INFO] \n",
            "Saving decoder embeddings as:\n",
            "\t* dec: /content/drive/MyDrive/EgyptianTranslation/word2vec/out.dec_embeddings.pt\n",
            "[2021-09-24 17:06:16,156 INFO]  * src vocab size = 8818\n",
            "[2021-09-24 17:06:16,156 INFO]  * tgt vocab size = 6360\n",
            "[2021-09-24 17:06:16,158 INFO] Building model...\n",
            "[2021-09-24 17:06:26,364 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8818, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6360, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=6360, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-09-24 17:06:26,399 INFO] encoder: 23430144\n",
            "[2021-09-24 17:06:26,400 INFO] decoder: 31744216\n",
            "[2021-09-24 17:06:26,400 INFO] * number of parameters: 55174360\n",
            "[2021-09-24 17:06:26,404 INFO] Starting training on GPU: [0]\n",
            "[2021-09-24 17:06:26,404 INFO] Start training loop and validate every 1500 steps...\n",
            "[2021-09-24 17:06:26,404 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-09-24 17:06:26,404 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2021-09-24 17:06:57,808 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2021-09-24 17:07:05,833 INFO] Step 50/ 3000; acc:   3.78; ppl: 1588.62; xent: 7.37; lr: 0.00001; 2087/2830 tok/s;     39 sec\n",
            "[2021-09-24 17:07:29,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2021-09-24 17:07:44,270 INFO] Step 100/ 3000; acc:  11.65; ppl: 807.59; xent: 6.69; lr: 0.00001; 2133/2860 tok/s;     78 sec\n",
            "[2021-09-24 17:08:07,344 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2021-09-24 17:08:23,191 INFO] Step 150/ 3000; acc:  11.44; ppl: 528.67; xent: 6.27; lr: 0.00002; 2126/2874 tok/s;    117 sec\n",
            "[2021-09-24 17:08:38,603 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2021-09-24 17:09:02,140 INFO] Step 200/ 3000; acc:  11.49; ppl: 363.03; xent: 5.89; lr: 0.00002; 2119/2873 tok/s;    156 sec\n",
            "[2021-09-24 17:09:16,938 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2021-09-24 17:09:42,410 INFO] Step 250/ 3000; acc:  12.24; ppl: 254.82; xent: 5.54; lr: 0.00003; 2144/2892 tok/s;    196 sec\n",
            "[2021-09-24 17:09:47,916 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2021-09-24 17:10:22,360 INFO] Step 300/ 3000; acc:  13.17; ppl: 191.35; xent: 5.25; lr: 0.00004; 2145/2891 tok/s;    236 sec\n",
            "[2021-09-24 17:10:26,399 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2021-09-24 17:10:57,525 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2021-09-24 17:11:02,786 INFO] Step 350/ 3000; acc:  13.83; ppl: 158.65; xent: 5.07; lr: 0.00004; 2134/2884 tok/s;    276 sec\n",
            "[2021-09-24 17:11:36,039 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2021-09-24 17:11:42,328 INFO] Step 400/ 3000; acc:  15.40; ppl: 141.19; xent: 4.95; lr: 0.00005; 2110/2851 tok/s;    316 sec\n",
            "[2021-09-24 17:12:07,329 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2021-09-24 17:12:21,183 INFO] Step 450/ 3000; acc:  16.55; ppl: 129.12; xent: 4.86; lr: 0.00006; 2129/2862 tok/s;    355 sec\n",
            "[2021-09-24 17:12:46,033 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2021-09-24 17:13:00,394 INFO] Step 500/ 3000; acc:  17.76; ppl: 115.87; xent: 4.75; lr: 0.00006; 2100/2834 tok/s;    394 sec\n",
            "[2021-09-24 17:13:17,328 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2021-09-24 17:13:39,838 INFO] Step 550/ 3000; acc:  18.69; ppl: 103.78; xent: 4.64; lr: 0.00007; 2098/2850 tok/s;    433 sec\n",
            "[2021-09-24 17:13:56,025 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2021-09-24 17:14:19,716 INFO] Step 600/ 3000; acc:  19.66; ppl: 93.44; xent: 4.54; lr: 0.00007; 2127/2869 tok/s;    473 sec\n",
            "[2021-09-24 17:14:27,293 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2021-09-24 17:15:00,151 INFO] Step 650/ 3000; acc:  21.08; ppl: 81.49; xent: 4.40; lr: 0.00008; 2150/2894 tok/s;    514 sec\n",
            "[2021-09-24 17:15:05,728 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2021-09-24 17:15:36,922 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2021-09-24 17:15:40,382 INFO] Step 700/ 3000; acc:  22.90; ppl: 71.16; xent: 4.26; lr: 0.00009; 2123/2881 tok/s;    554 sec\n",
            "[2021-09-24 17:16:15,330 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2021-09-24 17:16:19,985 INFO] Step 750/ 3000; acc:  24.22; ppl: 63.79; xent: 4.16; lr: 0.00009; 2124/2863 tok/s;    594 sec\n",
            "[2021-09-24 17:16:46,735 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2021-09-24 17:16:59,051 INFO] Step 800/ 3000; acc:  25.97; ppl: 56.11; xent: 4.03; lr: 0.00010; 2114/2851 tok/s;    633 sec\n",
            "[2021-09-24 17:17:25,410 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2021-09-24 17:17:38,374 INFO] Step 850/ 3000; acc:  27.42; ppl: 49.22; xent: 3.90; lr: 0.00011; 2116/2844 tok/s;    672 sec\n",
            "[2021-09-24 17:17:56,702 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2021-09-24 17:18:17,322 INFO] Step 900/ 3000; acc:  29.28; ppl: 42.82; xent: 3.76; lr: 0.00011; 2118/2870 tok/s;    711 sec\n",
            "[2021-09-24 17:18:35,213 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2021-09-24 17:18:57,009 INFO] Step 950/ 3000; acc:  30.71; ppl: 37.41; xent: 3.62; lr: 0.00012; 2120/2865 tok/s;    751 sec\n",
            "[2021-09-24 17:19:06,476 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2021-09-24 17:19:37,220 INFO] Step 1000/ 3000; acc:  32.98; ppl: 32.01; xent: 3.47; lr: 0.00012; 2147/2891 tok/s;    791 sec\n",
            "[2021-09-24 17:19:37,270 INFO] Saving checkpoint /content/drive/MyDrive/EgyptianTranslation/word2vec/output_step_1000.pt\n",
            "[2021-09-24 17:19:48,367 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2021-09-24 17:20:19,995 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2021-09-24 17:20:21,793 INFO] Step 1050/ 3000; acc:  35.18; ppl: 27.75; xent: 3.32; lr: 0.00013; 1950/2640 tok/s;    835 sec\n",
            "[2021-09-24 17:20:58,624 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2021-09-24 17:21:01,274 INFO] Step 1100/ 3000; acc:  37.32; ppl: 24.06; xent: 3.18; lr: 0.00014; 2115/2862 tok/s;    875 sec\n",
            "[2021-09-24 17:21:29,813 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2021-09-24 17:21:40,484 INFO] Step 1150/ 3000; acc:  39.28; ppl: 21.28; xent: 3.06; lr: 0.00014; 2110/2849 tok/s;    914 sec\n",
            "[2021-09-24 17:22:08,283 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2021-09-24 17:22:19,432 INFO] Step 1200/ 3000; acc:  41.31; ppl: 18.47; xent: 2.92; lr: 0.00015; 2135/2871 tok/s;    953 sec\n",
            "[2021-09-24 17:22:39,288 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2021-09-24 17:22:58,337 INFO] Step 1250/ 3000; acc:  43.21; ppl: 16.36; xent: 2.79; lr: 0.00015; 2133/2879 tok/s;    992 sec\n",
            "[2021-09-24 17:23:17,612 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2021-09-24 17:23:37,373 INFO] Step 1300/ 3000; acc:  45.69; ppl: 13.80; xent: 2.62; lr: 0.00016; 2128/2894 tok/s;   1031 sec\n",
            "[2021-09-24 17:23:48,732 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2021-09-24 17:24:17,702 INFO] Step 1350/ 3000; acc:  47.58; ppl: 12.04; xent: 2.49; lr: 0.00017; 2141/2870 tok/s;   1071 sec\n",
            "[2021-09-24 17:24:27,025 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2021-09-24 17:24:58,260 INFO] Step 1400/ 3000; acc:  50.40; ppl: 10.29; xent: 2.33; lr: 0.00017; 2154/2911 tok/s;   1112 sec\n",
            "[2021-09-24 17:24:58,267 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2021-09-24 17:25:36,797 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2021-09-24 17:25:38,238 INFO] Step 1450/ 3000; acc:  53.23; ppl:  8.80; xent: 2.17; lr: 0.00018; 2112/2866 tok/s;   1152 sec\n",
            "[2021-09-24 17:26:08,163 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2021-09-24 17:26:17,319 INFO] Step 1500/ 3000; acc:  55.14; ppl:  7.96; xent: 2.07; lr: 0.00019; 2103/2839 tok/s;   1191 sec\n",
            "[2021-09-24 17:26:17,321 INFO] valid's transforms: TransformPipe()\n",
            "[2021-09-24 17:26:19,308 INFO] Validation perplexity: 87.8647\n",
            "[2021-09-24 17:26:19,308 INFO] Validation accuracy: 32.1458\n",
            "[2021-09-24 17:26:48,628 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2021-09-24 17:26:58,458 INFO] Step 1550/ 3000; acc:  57.43; ppl:  6.96; xent: 1.94; lr: 0.00019; 2012/2699 tok/s;   1232 sec\n",
            "[2021-09-24 17:27:19,958 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2021-09-24 17:27:37,469 INFO] Step 1600/ 3000; acc:  58.96; ppl:  6.38; xent: 1.85; lr: 0.00020; 2122/2875 tok/s;   1271 sec\n",
            "[2021-09-24 17:27:58,453 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2021-09-24 17:28:16,793 INFO] Step 1650/ 3000; acc:  61.32; ppl:  5.62; xent: 1.73; lr: 0.00020; 2129/2875 tok/s;   1310 sec\n",
            "[2021-09-24 17:28:29,533 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2021-09-24 17:28:56,689 INFO] Step 1700/ 3000; acc:  64.13; ppl:  4.88; xent: 1.58; lr: 0.00021; 2135/2883 tok/s;   1350 sec\n",
            "[2021-09-24 17:29:07,970 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2021-09-24 17:29:36,959 INFO] Step 1750/ 3000; acc:  66.50; ppl:  4.37; xent: 1.48; lr: 0.00022; 2156/2902 tok/s;   1391 sec\n",
            "[2021-09-24 17:29:39,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2021-09-24 17:30:10,052 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2021-09-24 17:30:17,406 INFO] Step 1800/ 3000; acc:  69.28; ppl:  3.85; xent: 1.35; lr: 0.00022; 2137/2891 tok/s;   1431 sec\n",
            "[2021-09-24 17:30:48,212 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2021-09-24 17:30:56,127 INFO] Step 1850/ 3000; acc:  71.05; ppl:  3.59; xent: 1.28; lr: 0.00023; 2125/2881 tok/s;   1470 sec\n",
            "[2021-09-24 17:31:19,349 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2021-09-24 17:31:34,466 INFO] Step 1900/ 3000; acc:  73.09; ppl:  3.26; xent: 1.18; lr: 0.00023; 2139/2868 tok/s;   1508 sec\n",
            "[2021-09-24 17:31:57,525 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2021-09-24 17:32:13,323 INFO] Step 1950/ 3000; acc:  75.19; ppl:  3.00; xent: 1.10; lr: 0.00024; 2130/2879 tok/s;   1547 sec\n",
            "[2021-09-24 17:32:28,603 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2021-09-24 17:32:52,070 INFO] Step 2000/ 3000; acc:  76.95; ppl:  2.79; xent: 1.02; lr: 0.00025; 2130/2888 tok/s;   1586 sec\n",
            "[2021-09-24 17:32:52,123 INFO] Saving checkpoint /content/drive/MyDrive/EgyptianTranslation/word2vec/output_step_2000.pt\n",
            "[2021-09-24 17:33:10,252 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2021-09-24 17:33:35,766 INFO] Step 2050/ 3000; acc:  79.27; ppl:  2.55; xent: 0.93; lr: 0.00025; 1976/2666 tok/s;   1629 sec\n",
            "[2021-09-24 17:33:41,248 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2021-09-24 17:34:15,480 INFO] Step 2100/ 3000; acc:  81.61; ppl:  2.32; xent: 0.84; lr: 0.00026; 2158/2908 tok/s;   1669 sec\n",
            "[2021-09-24 17:34:19,526 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2021-09-24 17:34:50,711 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2021-09-24 17:34:55,959 INFO] Step 2150/ 3000; acc:  83.58; ppl:  2.17; xent: 0.78; lr: 0.00027; 2131/2881 tok/s;   1710 sec\n",
            "[2021-09-24 17:35:29,247 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2021-09-24 17:35:35,562 INFO] Step 2200/ 3000; acc:  84.98; ppl:  2.06; xent: 0.72; lr: 0.00027; 2107/2846 tok/s;   1749 sec\n",
            "[2021-09-24 17:36:00,460 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2021-09-24 17:36:14,309 INFO] Step 2250/ 3000; acc:  86.66; ppl:  1.94; xent: 0.66; lr: 0.00028; 2135/2870 tok/s;   1788 sec\n",
            "[2021-09-24 17:36:39,114 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2021-09-24 17:36:53,470 INFO] Step 2300/ 3000; acc:  87.60; ppl:  1.87; xent: 0.63; lr: 0.00028; 2102/2837 tok/s;   1827 sec\n",
            "[2021-09-24 17:37:10,395 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2021-09-24 17:37:32,829 INFO] Step 2350/ 3000; acc:  88.85; ppl:  1.79; xent: 0.58; lr: 0.00029; 2102/2857 tok/s;   1866 sec\n",
            "[2021-09-24 17:37:49,031 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2021-09-24 17:38:12,797 INFO] Step 2400/ 3000; acc:  89.92; ppl:  1.71; xent: 0.54; lr: 0.00030; 2122/2862 tok/s;   1906 sec\n",
            "[2021-09-24 17:38:20,412 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2021-09-24 17:38:53,354 INFO] Step 2450/ 3000; acc:  91.03; ppl:  1.64; xent: 0.50; lr: 0.00030; 2144/2885 tok/s;   1947 sec\n",
            "[2021-09-24 17:38:58,953 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2021-09-24 17:39:30,307 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2021-09-24 17:39:33,775 INFO] Step 2500/ 3000; acc:  91.96; ppl:  1.59; xent: 0.46; lr: 0.00031; 2113/2868 tok/s;   1987 sec\n",
            "[2021-09-24 17:40:08,836 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2021-09-24 17:40:13,523 INFO] Step 2550/ 3000; acc:  92.89; ppl:  1.53; xent: 0.43; lr: 0.00032; 2117/2852 tok/s;   2027 sec\n",
            "[2021-09-24 17:40:40,336 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2021-09-24 17:40:52,627 INFO] Step 2600/ 3000; acc:  93.68; ppl:  1.49; xent: 0.40; lr: 0.00032; 2112/2849 tok/s;   2066 sec\n",
            "[2021-09-24 17:41:19,036 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2021-09-24 17:41:32,032 INFO] Step 2650/ 3000; acc:  94.31; ppl:  1.46; xent: 0.38; lr: 0.00033; 2112/2838 tok/s;   2106 sec\n",
            "[2021-09-24 17:41:50,472 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2021-09-24 17:42:11,143 INFO] Step 2700/ 3000; acc:  94.74; ppl:  1.43; xent: 0.36; lr: 0.00033; 2109/2858 tok/s;   2145 sec\n",
            "[2021-09-24 17:42:29,104 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 63\n",
            "[2021-09-24 17:42:50,891 INFO] Step 2750/ 3000; acc:  95.08; ppl:  1.41; xent: 0.34; lr: 0.00034; 2117/2861 tok/s;   2184 sec\n",
            "[2021-09-24 17:43:00,391 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 64\n",
            "[2021-09-24 17:43:31,260 INFO] Step 2800/ 3000; acc:  95.42; ppl:  1.38; xent: 0.32; lr: 0.00035; 2139/2879 tok/s;   2225 sec\n",
            "[2021-09-24 17:43:38,990 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 65\n",
            "[2021-09-24 17:44:10,379 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 66\n",
            "[2021-09-24 17:44:12,186 INFO] Step 2850/ 3000; acc:  95.79; ppl:  1.36; xent: 0.31; lr: 0.00035; 2124/2875 tok/s;   2266 sec\n",
            "[2021-09-24 17:44:49,076 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 67\n",
            "[2021-09-24 17:44:51,705 INFO] Step 2900/ 3000; acc:  96.16; ppl:  1.34; xent: 0.29; lr: 0.00036; 2113/2859 tok/s;   2305 sec\n",
            "[2021-09-24 17:45:20,407 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 68\n",
            "[2021-09-24 17:45:31,060 INFO] Step 2950/ 3000; acc:  96.43; ppl:  1.33; xent: 0.28; lr: 0.00036; 2102/2838 tok/s;   2345 sec\n",
            "[2021-09-24 17:45:59,039 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 69\n",
            "[2021-09-24 17:46:10,273 INFO] Step 3000/ 3000; acc:  96.91; ppl:  1.30; xent: 0.26; lr: 0.00037; 2121/2851 tok/s;   2384 sec\n",
            "[2021-09-24 17:46:11,875 INFO] Validation perplexity: 87.0282\n",
            "[2021-09-24 17:46:11,875 INFO] Validation accuracy: 38.6673\n",
            "[2021-09-24 17:46:11,928 INFO] Saving checkpoint /content/drive/MyDrive/EgyptianTranslation/word2vec/output_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPZv8-MfJwUf",
        "outputId": "93bb2023-8c6a-446e-805e-4013c049fdd9"
      },
      "source": [
        "!onmt_translate -model EgyptianTranslation/word2vec/output_step_3000.pt -src /content/drive/MyDrive/EgyptianTranslation/compiled_corpora/aligned_test.egy.csv -output /content/drive/MyDrive/EgyptianTranslation/word2vec/pred_3000.txt\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-09-24 17:46:44,311 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "[2021-09-24 17:47:26,917 INFO] PRED AVG SCORE: -0.9507, PRED PPL: 2.5876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFNGcImY8V86",
        "outputId": "db682e1f-bb68-4537-9de1-cc0284e24035"
      },
      "source": [
        "ref_path = \"EgyptianTranslation/compiled_corpora/aligned_test.eng.csv\"\n",
        "with open(ref_path, 'r') as ref_file:\n",
        "  ref = [list(map(lambda x: x.strip('\\n'), ref_file.readlines()))]\n",
        "\n",
        "print(len(ref[0]))\n",
        "ref[0][:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beginning of the teaching made by the leader of nobility foremost of action made by a man for his son',\n",
              " 'father of the god beloved of the god',\n",
              " 'master of secrets of the house of the king may he live flourish and be well',\n",
              " 'overlord of the land to its limit',\n",
              " 'sem priest controller of the kilted',\n",
              " 'as a teaching before his children',\n",
              " 'let me say what is great may you listen',\n",
              " 'as i cause you to know the manner of eternity',\n",
              " 'a matter of live in truth',\n",
              " 'of proceeding to revered status']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uTi6z-j-St7",
        "outputId": "7c5a5ecc-7e49-4ab5-e656-72117d03799f"
      },
      "source": [
        "pred_path = \"EgyptianTranslation/word2vec/pred_3000.txt\"\n",
        "with open(pred_path, 'r') as pred_file:\n",
        "  pred = list(map(lambda x: x.strip('\\n'), pred_file.readlines()))\n",
        "\n",
        "print(len(pred))\n",
        "pred[:10]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beginning of the teaching of the prince regent count',\n",
              " 'gods father and beloved of the god',\n",
              " 'initiated of the palace and health',\n",
              " 'chief of the entire land',\n",
              " 'protector of the wretched',\n",
              " 'instruction of',\n",
              " 'i said i shall tell me a bird .',\n",
              " 'i let us our gift of eternity',\n",
              " 'a method that has been successful',\n",
              " 'and passing to blessedness']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbAfOQUi9jUm",
        "outputId": "e0c2dde8-147a-4816-c2ec-ece84c9aa761"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.2 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjpvrFF4RhzJ",
        "outputId": "da043b4e-ce0f-4a37-a700-bc72213f92c5"
      },
      "source": [
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "split_ref = [[item] for item in ref[0]]\n",
        "split_pred = [item for item in pred]\n",
        "bleu_score.corpus_bleu(split_ref, split_pred, smoothing_function=SmoothingFunction().method5) * 100"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.67720261904486"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCQJoH57DUK",
        "outputId": "15a53a34-fa1b-446d-89c6-f544cab91bd2"
      },
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "bleu = BLEU()\n",
        "split_ref = [[item] for item in ref[0]]\n",
        "split_pred = [item for item in pred]\n",
        "bleu.corpus_score(split_pred, split_ref)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 36.89 77.8/50.0/28.6/16.7 (BP = 1.000 ratio = 1.000 hyp_len = 9 ref_len = 9)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}